# Pentamind Architecture

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚                    ðŸ§  PENTAMIND SYSTEM                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       FRONTEND LAYER                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Tauri Desktop App (Frameless, Always-on-Top)    â”‚     â”‚
â”‚  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     â”‚
â”‚  â”‚                                                     â”‚     â”‚
â”‚  â”‚   React 19 + TypeScript + Tailwind CSS 4          â”‚     â”‚
â”‚  â”‚                                                     â”‚     â”‚
â”‚  â”‚   Components:                                      â”‚     â”‚
â”‚  â”‚   â€¢ PentamindOverlay    (main UI)                 â”‚     â”‚
â”‚  â”‚   â€¢ Timeline            (execution viz)            â”‚     â”‚
â”‚  â”‚   â€¢ Scoreboard          (model comparison)         â”‚     â”‚
â”‚  â”‚   â€¢ ResponseViewer      (result display)           â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  Features:                                                   â”‚
â”‚  â€¢ Global hotkey (Cmd+Shift+P)                             â”‚
â”‚  â€¢ 5 task types (Summarize, Research, Solve, Code, Rewrite)â”‚
â”‚  â€¢ 3 modes (Best, Fast, Cheap)                             â”‚
â”‚  â€¢ Live timeline with animations                            â”‚
â”‚  â€¢ Model scoreboard with force-rerun                        â”‚
â”‚  â€¢ Execution replay viewer                                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ HTTP POST
                              â”‚ http://localhost:8000/run_jury
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       BACKEND LAYER                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚            FastAPI Server (Python 3.11+)           â”‚     â”‚
â”‚  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     â”‚
â”‚  â”‚                                                     â”‚     â”‚
â”‚  â”‚   Endpoints:                                       â”‚     â”‚
â”‚  â”‚   â€¢ GET  /health          â†’ {"ok": true}          â”‚     â”‚
â”‚  â”‚   â€¢ POST /infer           â†’ Single model call      â”‚     â”‚
â”‚  â”‚   â€¢ POST /run_jury        â†’ LangGraph workflow     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Invoke
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LANGGRAPH WORKFLOW                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   START                                                      â”‚
â”‚     â”‚                                                        â”‚
â”‚     â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚ classify_task   â”‚  Analyze intent, format, confidence    â”‚
â”‚  â”‚ Model: llama3-8bâ”‚  Output: task_spec JSON                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚     â”‚                                                        â”‚
â”‚     â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚ choose_model    â”‚  Select best model based on:           â”‚
â”‚  â”‚                 â”‚  â€¢ task_spec.intent                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â€¢ Input length                        â”‚
â”‚     â”‚                  â€¢ Citations needed                    â”‚
â”‚     â”‚                                                        â”‚
â”‚     â”œâ”€ intent=="code"      â†’ llama3.3-70b-instruct          â”‚
â”‚     â”œâ”€ intent=="reasoning" â†’ deepseek-r1-distill-llama-70b  â”‚
â”‚     â”œâ”€ input_len > 10K    â†’ gemini-1.5-flash-latest         â”‚
â”‚     â”œâ”€ input_len > 50K    â†’ gemini-1.5-pro-latest           â”‚
â”‚     â””â”€ else               â†’ llama3.3-70b-instruct (fallback)â”‚
â”‚     â”‚                                                        â”‚
â”‚     â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚ execute         â”‚  Run selected model                    â”‚
â”‚  â”‚                 â”‚  + Perplexity search (if needed)       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚     â”‚                                                        â”‚
â”‚     â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚ verify          â”‚  Check output validity:                â”‚
â”‚  â”‚                 â”‚  â€¢ JSON parseable?                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â€¢ Diff format correct?                â”‚
â”‚     â”‚                  â€¢ Non-empty?                          â”‚
â”‚     â”‚                                                        â”‚
â”‚     â”œâ”€ Valid â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚     â”‚                                           â”‚            â”‚
â”‚     â–¼                                           â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         END           â”‚
â”‚  â”‚ fallback        â”‚                                        â”‚
â”‚  â”‚ Use: llama3.3   â”‚  (Winner result returned)             â”‚
â”‚  â”‚ -70b-instruct   â”‚                                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚     â”‚                                                        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚                                                 â”‚            â”‚
â”‚                                                 â–¼            â”‚
â”‚                                               END           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ API Calls
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MODEL PROVIDERS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚      DigitalOcean Serverless Inference   â”‚               â”‚
â”‚  â”‚   https://inference.do-ai.run/v1         â”‚               â”‚
â”‚  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚               â”‚
â”‚  â”‚   â€¢ llama3-8b-instruct       (router)    â”‚               â”‚
â”‚  â”‚   â€¢ llama3.3-70b-instruct    (coding)    â”‚               â”‚
â”‚  â”‚   â€¢ deepseek-r1-distill-70b  (reasoning) â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚        Google Gemini API                 â”‚               â”‚
â”‚  â”‚   generativelanguage.googleapis.com      â”‚               â”‚
â”‚  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚               â”‚
â”‚  â”‚   â€¢ gemini-1.5-flash-latest (1M tokens)  â”‚               â”‚
â”‚  â”‚   â€¢ gemini-1.5-pro-latest   (2M tokens)  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚        Perplexity Search API             â”‚               â”‚
â”‚  â”‚   api.perplexity.ai                      â”‚               â”‚
â”‚  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚               â”‚
â”‚  â”‚   â€¢ Web search with citations            â”‚               â”‚
â”‚  â”‚   â€¢ Real-time information                â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow

### Request Flow:

```
User Input
   â”‚
   â”‚ 1. User types text
   â”‚ 2. Selects task type (e.g., Code)
   â”‚ 3. Chooses mode (e.g., Best)
   â”‚ 4. Clicks "Run Analysis"
   â”‚
   â–¼
Frontend (React)
   â”‚
   â”‚ POST /run_jury
   â”‚ {
   â”‚   "task": "code",
   â”‚   "input": "Write a quicksort",
   â”‚   "mode": "best"
   â”‚ }
   â”‚
   â–¼
Backend (FastAPI)
   â”‚
   â”‚ Invoke LangGraph
   â”‚
   â–¼
LangGraph Workflow
   â”‚
   â”œâ”€ 1. classify_task
   â”‚    â”œâ”€ Call: llama3-8b-instruct
   â”‚    â””â”€ Return: {"intent":"code","format":"text",...}
   â”‚
   â”œâ”€ 2. choose_model
   â”‚    â””â”€ Logic: intent=="code" â†’ llama3.3-70b-instruct
   â”‚
   â”œâ”€ 3. execute
   â”‚    â”œâ”€ Call: llama3.3-70b-instruct
   â”‚    â””â”€ Return: code implementation
   â”‚
   â”œâ”€ 4. verify
   â”‚    â””â”€ Check: output valid? âœ…
   â”‚
   â””â”€ END
   â”‚
   â–¼
Response
   â”‚
   â”‚ {
   â”‚   "final": "def quicksort(arr): ...",
   â”‚   "winner_model": "llama3.3-70b-instruct",
   â”‚   "task_spec": {...},
   â”‚   "scoreboard": [...],
   â”‚   "trace": [...]
   â”‚ }
   â”‚
   â–¼
Frontend Display
   â”‚
   â”œâ”€ Timeline: Show all steps with latencies
   â”œâ”€ Response: Display generated code
   â”œâ”€ Scoreboard: Show model comparison
   â””â”€ Replay: Full trace available
```

---

## Component Responsibilities

### Frontend Components:

| Component              | Responsibility                              |
| ---------------------- | ------------------------------------------- |
| `App.tsx`              | Main app logic, state management, API calls |
| `PentamindOverlay.tsx` | Layout, input handling, UI orchestration    |
| `Timeline.tsx`         | Visualize execution steps with animations   |
| `Scoreboard.tsx`       | Display model comparison, handle reruns     |
| `ResponseViewer.tsx`   | Show final result, task analysis, winner    |

### Backend Modules:

| Module                 | Responsibility                       |
| ---------------------- | ------------------------------------ |
| `main.py`              | FastAPI server, route definitions    |
| `types.py`             | Pydantic models for request/response |
| `langgraph_flow.py`    | Workflow logic, node definitions     |
| `call_model.py`        | DigitalOcean API client              |
| `gemini_client.py`     | Google Gemini API client             |
| `perplexity_search.py` | Perplexity search integration        |

---

## Model Selection Logic

```python
def choose_model(state):
    task_spec = state["task_spec"]
    input_text = state["input"]

    # Long context â†’ Gemini
    if len(input_text) > 50000:
        return "gemini-1.5-pro-latest"
    elif len(input_text) > 10000:
        return "gemini-1.5-flash-latest"

    # Intent-based routing
    if task_spec["intent"] == "code":
        return "llama3.3-70b-instruct"
    elif task_spec["intent"] == "reasoning":
        return "deepseek-r1-distill-llama-70b"
    else:
        return "llama3.3-70b-instruct"  # fallback
```

---

## Error Handling

### Frontend Errors:

```
Network Error â†’ Show red alert banner
Timeout       â†’ Show "Request timed out" message
API 500       â†’ Show "Backend error" + log details
```

### Backend Errors:

```
Model API Error â†’ Try fallback model
Timeout         â†’ Return error with trace so far
Invalid Input   â†’ Return 422 with validation errors
Missing API Key â†’ Graceful fallback (skip optional features)
```

---

## State Management

### Frontend State:

```typescript
{
  isVisible: boolean,           // Overlay shown?
  inputText: string,            // User input
  selectedTask: string,         // "code"|"research"|...
  selectedMode: "best"|"fast"|"cheap",
  isProcessing: boolean,        // Currently running?
  response: JuryResponse | null, // Result
  error: string | null          // Error message
}
```

### Backend State (LangGraph):

```python
{
  "task": str,                  # Original task type
  "input": str,                 # User input text
  "mode": str,                  # "best"|"fast"|"cheap"
  "task_spec": dict,            # Classification result
  "chosen_model": str,          # Selected model name
  "execution_output": str,      # Model response
  "verification_ok": bool,      # Passed verification?
  "final": str,                 # Final output
  "winner_model": str,          # Winning model
  "trace": List[dict]           # Execution trace
}
```

---

## Performance

### Latencies (typical):

| Step                   | Time             |
| ---------------------- | ---------------- |
| classify_task          | 100-200ms        |
| choose_model           | <10ms            |
| execute (code)         | 500-2000ms       |
| execute (reasoning)    | 1000-5000ms      |
| execute (long context) | 2000-10000ms     |
| verify                 | <50ms            |
| **Total**              | **1-12 seconds** |

### Optimizations:

- **Router model**: Use fast Llama 8B (not GPT-4)
- **Streaming**: Could add SSE for real-time updates
- **Caching**: Could cache task classifications
- **Parallel calls**: Could call multiple models in parallel

---

## Security

### API Keys:

- Stored in environment variables
- Never logged or exposed in traces
- Backend validates presence before calls

### CORS:

- Backend allows `localhost:1420` (Tauri dev)
- Production should restrict origins

### Input Validation:

- Pydantic models validate all inputs
- Max input length checks
- Sanitize outputs before display

---

## Deployment

### Backend:

```bash
# Local
uvicorn main:app --reload

# Production (DigitalOcean App Platform)
- Push to GitHub
- Connect DO App Platform
- Set env vars in dashboard
- Auto-deploy on push
```

### Frontend:

```bash
# Development
npm run tauri dev

# Build distributable
npm run tauri build

# Output:
# - macOS: .app bundle
# - Windows: .exe installer
# - Linux: .AppImage/.deb
```

---

## Tech Stack Summary

```
Frontend:
â”œâ”€â”€ Tauri 2              (Desktop framework)
â”œâ”€â”€ React 19             (UI library)
â”œâ”€â”€ TypeScript           (Type safety)
â”œâ”€â”€ Tailwind CSS 4       (Styling)
â””â”€â”€ Lucide React         (Icons)

Backend:
â”œâ”€â”€ Python 3.11+         (Language)
â”œâ”€â”€ FastAPI              (Web framework)
â”œâ”€â”€ LangGraph            (Workflow orchestration)
â”œâ”€â”€ Pydantic             (Data validation)
â”œâ”€â”€ httpx/requests       (HTTP clients)
â”œâ”€â”€ uvicorn              (ASGI server)
â””â”€â”€ pytest               (Testing)

AI Models:
â”œâ”€â”€ DigitalOcean Inference (Llama, DeepSeek)
â”œâ”€â”€ Google Gemini          (Long context)
â””â”€â”€ Perplexity             (Web search)
```

---

## File Structure

```
hack-mlhdo/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ types.py                  # Data models
â”‚   â”‚   â”œâ”€â”€ call_model.py             # DO API client
â”‚   â”‚   â”œâ”€â”€ gemini_client.py          # Gemini client
â”‚   â”‚   â”œâ”€â”€ perplexity_search.py      # Perplexity client
â”‚   â”‚   â””â”€â”€ langgraph_flow.py         # Workflow logic
â”‚   â”œâ”€â”€ main.py                        # FastAPI app
â”‚   â”œâ”€â”€ requirements.txt               # Python deps
â”‚   â”œâ”€â”€ test_api.py                    # Basic tests
â”‚   â”œâ”€â”€ test_real_tasks.py             # Real-world tests
â”‚   â”œâ”€â”€ test_perplexity.py             # Perplexity tests
â”‚   â”œâ”€â”€ test_gemini.py                 # Gemini tests
â”‚   â””â”€â”€ README.md                      # Backend docs
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx                    # Main app
â”‚   â”‚   â”œâ”€â”€ App.css                    # Styles
â”‚   â”‚   â”œâ”€â”€ main.tsx                   # Entry point
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ PentamindOverlay.tsx   # Main overlay
â”‚   â”‚       â”œâ”€â”€ Timeline.tsx           # Execution viz
â”‚   â”‚       â”œâ”€â”€ Scoreboard.tsx         # Model comparison
â”‚   â”‚       â””â”€â”€ ResponseViewer.tsx     # Result display
â”‚   â”œâ”€â”€ src-tauri/
â”‚   â”‚   â”œâ”€â”€ tauri.conf.json            # Window config
â”‚   â”‚   â””â”€â”€ src/main.rs                # Rust entry
â”‚   â”œâ”€â”€ package.json                    # Node deps
â”‚   â”œâ”€â”€ tailwind.config.js              # Tailwind config
â”‚   â”œâ”€â”€ README.md                       # Frontend docs
â”‚   â”œâ”€â”€ SETUP.md                        # Setup guide
â”‚   â”œâ”€â”€ QUICKSTART.md                   # Quick start
â”‚   â””â”€â”€ UI_GUIDE.md                     # UI reference
â”‚
â”œâ”€â”€ launch.sh                           # Complete launcher
â”œâ”€â”€ START_HERE.md                       # Main entry point
â”œâ”€â”€ PENTAMIND_OVERVIEW.md               # System overview
â””â”€â”€ ARCHITECTURE.md                     # This file
```

---

**Pentamind: Five Models, One Mind** ðŸ§ âœ¨

**Complete, tested, and ready for demo!** ðŸš€
